# üìò Documentation Backend - Int√©gration du Syst√®me Instagram Coach

**Pour :** D√©veloppeur Backend
**Version :** 1.0
**Derni√®re mise √† jour :** 2026-01-17

---

## üéØ Vue d'Ensemble

Ce syst√®me fournit un agent IA sp√©cialis√© en coaching Instagram avec **4 modes d'expertise** :
1. **Analyse de Performance** (content_analyst)
2. **Mon√©tisation** (monetization)
3. **Strat√©gie de Contenu** (strategy)
4. **Analyse d'Audience** (audience)

Le syst√®me utilise **RAG (Retrieval-Augmented Generation)** pour analyser les posts Instagram et fournir des r√©ponses bas√©es sur les donn√©es r√©elles.

---

## üì¶ Installation

```bash
# 1. Installer les d√©pendances
pip install -r requirements.txt

# 2. Configurer les variables d'environnement
cp .env.example .env

# 3. √âditer .env avec vos cl√©s API
# FEATHERLESS_API_KEY=votre_cl√©_ici
```

---

## üöÄ Quick Start

### Utilisation Basique

```python
from agent import InstagramCoachAgent, AgentMode

# Initialiser l'agent
agent = InstagramCoachAgent()

# Poser une question (sans streaming)
response = agent.ask(
    question="Quels sont mes meilleurs posts ?",
    mode=AgentMode.CONTENT_ANALYST,
    stream=False  # IMPORTANT pour r√©cup√©rer la r√©ponse compl√®te
)

print(response)  # String de la r√©ponse
```

### Avec Streaming (Recommand√©)

```python
# Poser une question avec streaming
result = agent.ask(
    question="Quels sont mes meilleurs posts ?",
    mode=AgentMode.CONTENT_ANALYST,
    stream=True  # Active le streaming
)

# result contient un g√©n√©rateur
# Attention : dans ask(), le streaming est d√©j√† g√©r√© et affich√©
# Pour une API, utilisez la fonction de bas niveau (voir section API)
```

---

## üèóÔ∏è Architecture du Syst√®me

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (React/Vue/etc)                 ‚îÇ
‚îÇ          Envoie: {question, mode, stream: true/false}       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ HTTP/WebSocket
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (FastAPI/Flask)                  ‚îÇ
‚îÇ  ‚Ä¢ Re√ßoit la requ√™te                                        ‚îÇ
‚îÇ  ‚Ä¢ Appelle InstagramCoachAgent.ask() ou generate_with_mode()‚îÇ
‚îÇ  ‚Ä¢ G√®re le streaming si n√©cessaire                          ‚îÇ
‚îÇ  ‚Ä¢ Retourne la r√©ponse                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ Appel Python
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SYST√àME INSTAGRAM COACH (Ce Code)              ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  InstagramCoachAgent (agent.py)                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Interface principale                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ G√®re les modes                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ M√©thode ask(question, mode, stream)              ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                      ‚îÇ                                      ‚îÇ
‚îÇ                      ‚ñº                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  agent_modes.py                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ generate_with_mode(rag_pipeline, question, mode) ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Charge le prompt sp√©cialis√©                      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                      ‚îÇ                                      ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ         ‚ñº                         ‚ñº                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ  ‚îÇ RAG Pipeline‚îÇ         ‚îÇ  PromptManager   ‚îÇ             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ R√©cup√®re  ‚îÇ         ‚îÇ  ‚Ä¢ Charge prompts‚îÇ             ‚îÇ
‚îÇ  ‚îÇ   les posts ‚îÇ         ‚îÇ  ‚Ä¢ Ajoute context‚îÇ             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Vectorise ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ  ‚îÇ ‚Ä¢ Query DB  ‚îÇ                                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îÇ         ‚îÇ                                                  ‚îÇ
‚îÇ         ‚ñº                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                          ‚îÇ
‚îÇ  ‚îÇ  LLM Client ‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Appelle  ‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ    l'API LLM‚îÇ                                          ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Streaming‚îÇ                                          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîå Int√©gration Backend

### Option 1 : API REST avec FastAPI (Sans Streaming)

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import sys
from pathlib import Path

# Import du syst√®me Instagram Coach
sys.path.insert(0, str(Path(__file__).parent / "src"))
from agent import InstagramCoachAgent, AgentMode

app = FastAPI()

# Initialiser l'agent au d√©marrage (singleton)
agent = InstagramCoachAgent()

# Mod√®le de requ√™te
class QuestionRequest(BaseModel):
    question: str
    mode: str = "content_analyst"  # d√©faut
    n_posts: int = 3

# Mod√®le de r√©ponse
class QuestionResponse(BaseModel):
    question: str
    mode: str
    response: str

# Mapping mode string ‚Üí enum
MODE_MAP = {
    "content_analyst": AgentMode.CONTENT_ANALYST,
    "monetization": AgentMode.MONETIZATION,
    "strategy": AgentMode.STRATEGY,
    "audience": AgentMode.AUDIENCE,
}

@app.post("/ask")
async def ask_question(req: QuestionRequest):
    """Endpoint pour poser une question"""

    # Valider le mode
    if req.mode not in MODE_MAP:
        raise HTTPException(400, f"Mode invalide: {req.mode}")

    try:
        # Appeler l'agent SANS streaming
        response = agent.ask(
            question=req.question,
            mode=MODE_MAP[req.mode],
            n_posts=req.n_posts,
            stream=False  # ‚Üê IMPORTANT : d√©sactive le streaming
        )

        return QuestionResponse(
            question=req.question,
            mode=req.mode,
            response=response
        )

    except Exception as e:
        raise HTTPException(500, str(e))

# Lancer avec: uvicorn main:app --reload
```

**Tester :**
```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quels sont mes meilleurs posts ?",
    "mode": "content_analyst"
  }'
```

---

### Option 2 : API REST avec Streaming (Server-Sent Events)

**IMPORTANT :** Le streaming est crucial pour une bonne UX car les r√©ponses peuvent prendre du temps.

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "src"))
from agent_modes import generate_with_mode, AgentMode
from rag_pipeline import get_rag_pipeline

app = FastAPI()

# Initialiser le pipeline RAG (singleton)
rag_pipeline = get_rag_pipeline()
rag_pipeline.load_data()

MODE_MAP = {
    "content_analyst": AgentMode.CONTENT_ANALYST,
    "monetization": AgentMode.MONETIZATION,
    "strategy": AgentMode.STRATEGY,
    "audience": AgentMode.AUDIENCE,
}

class QuestionRequest(BaseModel):
    question: str
    mode: str = "content_analyst"
    n_posts: int = 3

@app.post("/ask/stream")
async def ask_with_stream(req: QuestionRequest):
    """
    Endpoint avec streaming via Server-Sent Events (SSE).
    Le front re√ßoit la r√©ponse chunk par chunk en temps r√©el.
    """

    async def event_generator():
        """G√©n√©rateur pour SSE"""
        try:
            # Appeler generate_with_mode avec streaming
            result = generate_with_mode(
                rag_pipeline=rag_pipeline,
                question=req.question,
                mode=MODE_MAP.get(req.mode, AgentMode.CONTENT_ANALYST),
                n_results=req.n_posts,
                stream=True  # ‚Üê Active le streaming
            )

            # Envoyer les m√©tadonn√©es d'abord
            yield f"data: {json.dumps({'type': 'metadata', 'mode': req.mode})}\n\n"

            # Streamer la r√©ponse chunk par chunk
            for chunk in result['response_stream']:
                # Format SSE
                data = json.dumps({
                    'type': 'chunk',
                    'content': chunk
                })
                yield f"data: {data}\n\n"

            # Signal de fin
            yield f"data: {json.dumps({'type': 'done'})}\n\n"

        except Exception as e:
            # En cas d'erreur
            error_data = json.dumps({
                'type': 'error',
                'message': str(e)
            })
            yield f"data: {error_data}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # Pour nginx
        }
    )

# Lancer avec: uvicorn main:app --reload
```

**Consommer c√¥t√© Frontend (JavaScript) :**

```javascript
const eventSource = new EventSource('http://localhost:8000/ask/stream', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    question: "Quels sont mes meilleurs posts ?",
    mode: "content_analyst"
  })
});

let fullResponse = "";

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);

  if (data.type === 'chunk') {
    fullResponse += data.content;
    // Afficher le chunk dans l'UI
    updateUI(data.content);
  } else if (data.type === 'done') {
    console.log("Streaming termin√©");
    eventSource.close();
  } else if (data.type === 'error') {
    console.error("Erreur:", data.message);
    eventSource.close();
  }
};
```

---

### Option 3 : WebSocket (Streaming Bidirectionnel)

```python
from fastapi import FastAPI, WebSocket
from fastapi.websockets import WebSocketDisconnect
import json
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "src"))
from agent_modes import generate_with_mode, AgentMode
from rag_pipeline import get_rag_pipeline

app = FastAPI()
rag_pipeline = get_rag_pipeline()
rag_pipeline.load_data()

MODE_MAP = {
    "content_analyst": AgentMode.CONTENT_ANALYST,
    "monetization": AgentMode.MONETIZATION,
    "strategy": AgentMode.STRATEGY,
    "audience": AgentMode.AUDIENCE,
}

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """WebSocket pour chat en temps r√©el avec streaming"""
    await websocket.accept()

    try:
        while True:
            # Recevoir la question du client
            data = await websocket.receive_text()
            request = json.loads(data)

            question = request.get('question')
            mode = request.get('mode', 'content_analyst')
            n_posts = request.get('n_posts', 3)

            # Envoyer confirmation de r√©ception
            await websocket.send_json({
                'type': 'processing',
                'message': 'Analyse en cours...'
            })

            # G√©n√©rer avec streaming
            result = generate_with_mode(
                rag_pipeline=rag_pipeline,
                question=question,
                mode=MODE_MAP.get(mode, AgentMode.CONTENT_ANALYST),
                n_results=n_posts,
                stream=True
            )

            # Streamer chaque chunk
            for chunk in result['response_stream']:
                await websocket.send_json({
                    'type': 'chunk',
                    'content': chunk
                })

            # Signal de fin
            await websocket.send_json({
                'type': 'done'
            })

    except WebSocketDisconnect:
        print("Client d√©connect√©")
    except Exception as e:
        await websocket.send_json({
            'type': 'error',
            'message': str(e)
        })
```

**Consommer c√¥t√© Frontend (JavaScript) :**

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/chat');

ws.onopen = () => {
  // Envoyer une question
  ws.send(JSON.stringify({
    question: "Quels sont mes meilleurs posts ?",
    mode: "content_analyst",
    n_posts: 5
  }));
};

let fullResponse = "";

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);

  switch(data.type) {
    case 'processing':
      console.log("En cours...");
      break;
    case 'chunk':
      fullResponse += data.content;
      updateUI(data.content);  // Afficher chunk par chunk
      break;
    case 'done':
      console.log("Termin√© !");
      break;
    case 'error':
      console.error("Erreur:", data.message);
      break;
  }
};
```

---

## üìä Utilisation de Bas Niveau (Recommand√© pour Backend)

Pour plus de contr√¥le, utilisez directement `generate_with_mode()` :

```python
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent / "src"))

from agent_modes import generate_with_mode, AgentMode
from rag_pipeline import get_rag_pipeline

# Initialiser le pipeline RAG (faire une seule fois au d√©marrage)
rag_pipeline = get_rag_pipeline()
rag_pipeline.load_data()

# Fonction pour g√©n√©rer une r√©ponse
def get_instagram_advice(question: str, mode: str = "content_analyst", stream: bool = False):
    """
    G√©n√®re un conseil Instagram.

    Args:
        question: Question de l'utilisateur
        mode: Mode d'agent (content_analyst, monetization, strategy, audience)
        stream: Si True, retourne un g√©n√©rateur pour streaming

    Returns:
        Si stream=False: dict avec 'response' (string)
        Si stream=True: dict avec 'response_stream' (generator)
    """
    mode_map = {
        "content_analyst": AgentMode.CONTENT_ANALYST,
        "monetization": AgentMode.MONETIZATION,
        "strategy": AgentMode.STRATEGY,
        "audience": AgentMode.AUDIENCE,
    }

    result = generate_with_mode(
        rag_pipeline=rag_pipeline,
        question=question,
        mode=mode_map.get(mode, AgentMode.CONTENT_ANALYST),
        n_results=3,  # Nombre de posts √† analyser
        temperature=0.5,  # Cr√©ativit√© (0-1)
        max_tokens=1000,  # Longueur max de la r√©ponse
        stream=stream
    )

    return result

# Exemple sans streaming
result = get_instagram_advice(
    "Quels sont mes meilleurs posts ?",
    mode="content_analyst",
    stream=False
)
print(result['response'])

# Exemple avec streaming
result = get_instagram_advice(
    "Comment mon√©tiser mon compte ?",
    mode="monetization",
    stream=True
)
for chunk in result['response_stream']:
    print(chunk, end='', flush=True)
```

---

## üéõÔ∏è Les 4 Modes Disponibles

### 1. content_analyst (Analyse de Performance)
**Quand l'utiliser :** Questions sur m√©triques, performance, engagement

```python
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question="Quels sont mes posts les plus performants ?",
    mode=AgentMode.CONTENT_ANALYST
)
```

**Exemples de questions :**
- "Quels sont mes meilleurs posts ?"
- "Pourquoi mon engagement a baiss√© ?"
- "Quel type de contenu fonctionne le mieux ?"

---

### 2. monetization (Mon√©tisation)
**Quand l'utiliser :** Questions sur revenus, partenariats, prix

```python
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question="Combien facturer pour un post sponsoris√© ?",
    mode=AgentMode.MONETIZATION
)
```

**Exemples de questions :**
- "Combien facturer pour un post sponsoris√© ?"
- "Quelles marques devrais-je contacter ?"
- "Comment diversifier mes revenus ?"

---

### 3. strategy (Strat√©gie de Contenu)
**Quand l'utiliser :** Questions sur planning, id√©es, cr√©ation

```python
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question="Quelles id√©es de contenu pour cette semaine ?",
    mode=AgentMode.STRATEGY
)
```

**Exemples de questions :**
- "Quelles id√©es de contenu pour cette semaine ?"
- "Comment planifier mon calendrier √©ditorial ?"
- "Comment rendre mon contenu viral ?"

---

### 4. audience (Analyse d'Audience)
**Quand l'utiliser :** Questions sur communaut√©, followers, d√©mographie

```python
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question="Qui sont mes followers ?",
    mode=AgentMode.AUDIENCE
)
```

**Exemples de questions :**
- "Qui sont vraiment mes followers ?"
- "Mon audience ach√®terait-elle mes produits ?"
- "Comment attirer plus de followers qualifi√©s ?"

---

## üîß Param√®tres Disponibles

### `generate_with_mode()` - Fonction de Bas Niveau

```python
from agent_modes import generate_with_mode, AgentMode

result = generate_with_mode(
    rag_pipeline,      # Instance du RAG pipeline
    question="...",    # Question de l'utilisateur
    mode=AgentMode.CONTENT_ANALYST,  # Mode d'agent
    n_results=3,       # Nombre de posts √† r√©cup√©rer (3-10 recommand√©)
    temperature=0.5,   # Cr√©ativit√©: 0=d√©terministe, 1=cr√©atif
    max_tokens=1000,   # Longueur max de la r√©ponse
    stream=False       # True pour streaming, False pour r√©ponse compl√®te
)
```

**Retour si `stream=False` :**
```python
{
    "mode": "content_analyst",
    "mode_description": "üìä Analyse de Performance...",
    "response": "Voici vos meilleurs posts...",  # String compl√®te
    "relevant_posts": [...],  # Posts utilis√©s pour la r√©ponse
    "question": "..."
}
```

**Retour si `stream=True` :**
```python
{
    "mode": "content_analyst",
    "mode_description": "üìä Analyse de Performance...",
    "response_stream": <generator>,  # G√©n√©rateur de chunks
    "relevant_posts": [...],
    "question": "..."
}

# Utilisation du g√©n√©rateur
for chunk in result['response_stream']:
    print(chunk, end='', flush=True)
```

---

## üìÅ Structure des Donn√©es

### Format de Requ√™te (Frontend ‚Üí Backend)

```json
{
  "question": "Quels sont mes meilleurs posts ?",
  "mode": "content_analyst",
  "n_posts": 5,
  "stream": true
}
```

### Format de R√©ponse (Backend ‚Üí Frontend)

**Sans streaming :**
```json
{
  "question": "Quels sont mes meilleurs posts ?",
  "mode": "content_analyst",
  "mode_description": "üìä Analyse de Performance",
  "response": "D'apr√®s l'analyse de vos posts...",
  "n_posts_analyzed": 5
}
```

**Avec streaming (SSE) :**
```
data: {"type": "metadata", "mode": "content_analyst"}

data: {"type": "chunk", "content": "D'apr√®s"}

data: {"type": "chunk", "content": " l'analyse"}

data: {"type": "chunk", "content": " de vos"}

...

data: {"type": "done"}
```

---

## ‚öôÔ∏è Configuration

### Variables d'Environnement (.env)

```bash
# API Key pour le LLM (Featherless)
FEATHERLESS_API_KEY=your_api_key_here

# Optionnel : Chemins des donn√©es
POSTS_DATA_PATH=data/posts.json
PROFILE_DATA_PATH=data/profile.json

# Optionnel : Configuration LLM
LLM_MODEL=meta-llama/Meta-Llama-3.1-405B-Instruct
LLM_TEMPERATURE=0.5
LLM_MAX_TOKENS=1000
```

### Charger les Donn√©es Instagram

```python
from rag_pipeline import get_rag_pipeline

# M√©thode 1 : Chargement automatique (depuis config)
rag_pipeline = get_rag_pipeline()
rag_pipeline.load_data()

# M√©thode 2 : Chemins personnalis√©s
rag_pipeline.load_data(
    posts_path="custom/path/posts.json",
    profile_path="custom/path/profile.json"
)

# M√©thode 3 : Recharger les donn√©es
rag_pipeline.load_data(force_reload=True)
```

---

## üîí Gestion d'Erreurs

```python
from fastapi import HTTPException

@app.post("/ask")
async def ask(req: QuestionRequest):
    try:
        # Validation du mode
        if req.mode not in MODE_MAP:
            raise HTTPException(
                status_code=400,
                detail=f"Mode invalide. Modes valides: {list(MODE_MAP.keys())}"
            )

        # Validation de la question
        if not req.question or len(req.question) < 3:
            raise HTTPException(
                status_code=400,
                detail="Question trop courte"
            )

        # G√©n√©ration de la r√©ponse
        result = generate_with_mode(...)

        return result

    except FileNotFoundError as e:
        # Donn√©es Instagram manquantes
        raise HTTPException(
            status_code=503,
            detail="Donn√©es Instagram non disponibles. Veuillez charger les donn√©es."
        )

    except Exception as e:
        # Erreur g√©n√©rique
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne: {str(e)}"
        )
```

---

## üö¶ Performance et Optimisation

### 1. Initialisation au D√©marrage (IMPORTANT)

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI

# Singleton du pipeline RAG
rag_pipeline = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialiser au d√©marrage, nettoyer √† l'arr√™t"""
    global rag_pipeline

    # Startup
    print("üöÄ Chargement du mod√®le et des donn√©es...")
    rag_pipeline = get_rag_pipeline()
    rag_pipeline.load_data()
    print("‚úÖ Pr√™t !")

    yield

    # Shutdown
    print("üëã Nettoyage...")

app = FastAPI(lifespan=lifespan)

# Les endpoints utilisent le rag_pipeline global
@app.post("/ask")
async def ask(req: QuestionRequest):
    result = generate_with_mode(
        rag_pipeline=rag_pipeline,  # ‚Üê Utilise l'instance globale
        ...
    )
    return result
```

### 2. Cache des R√©sultats

```python
from functools import lru_cache
import hashlib

# Cache simple en m√©moire
@lru_cache(maxsize=100)
def get_cached_response(question: str, mode: str):
    """Cache les r√©ponses pour les questions fr√©quentes"""
    result = generate_with_mode(
        rag_pipeline=rag_pipeline,
        question=question,
        mode=MODE_MAP[mode],
        stream=False
    )
    return result['response']

# Utilisation
response = get_cached_response(
    question="Quels sont mes meilleurs posts ?",
    mode="content_analyst"
)
```

### 3. R√©glage des Param√®tres

```python
# Pour des r√©ponses RAPIDES mais moins d√©taill√©es
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question=question,
    mode=mode,
    n_results=3,        # Moins de posts = plus rapide
    max_tokens=500,     # R√©ponse plus courte
    temperature=0.3     # Moins cr√©atif mais plus rapide
)

# Pour des r√©ponses D√âTAILL√âES mais plus lentes
result = generate_with_mode(
    rag_pipeline=rag_pipeline,
    question=question,
    mode=mode,
    n_results=10,       # Plus de contexte
    max_tokens=2000,    # R√©ponse longue
    temperature=0.7     # Plus cr√©atif
)
```

---

## üß™ Tests

### Test Unitaire

```python
import pytest
from agent_modes import generate_with_mode, AgentMode
from rag_pipeline import get_rag_pipeline

@pytest.fixture(scope="module")
def rag():
    """Initialiser le pipeline une fois pour tous les tests"""
    pipeline = get_rag_pipeline()
    pipeline.load_data()
    return pipeline

def test_content_analyst_mode(rag):
    """Test du mode analyse de contenu"""
    result = generate_with_mode(
        rag_pipeline=rag,
        question="Quels sont mes meilleurs posts ?",
        mode=AgentMode.CONTENT_ANALYST,
        stream=False
    )

    assert 'response' in result
    assert isinstance(result['response'], str)
    assert len(result['response']) > 0
    assert result['mode'] == 'content_analyst'

def test_monetization_mode(rag):
    """Test du mode mon√©tisation"""
    result = generate_with_mode(
        rag_pipeline=rag,
        question="Combien facturer ?",
        mode=AgentMode.MONETIZATION,
        stream=False
    )

    assert 'response' in result
    assert result['mode'] == 'monetization'

def test_streaming(rag):
    """Test du streaming"""
    result = generate_with_mode(
        rag_pipeline=rag,
        question="Test streaming",
        mode=AgentMode.CONTENT_ANALYST,
        stream=True
    )

    assert 'response_stream' in result

    # Consommer le stream
    chunks = list(result['response_stream'])
    assert len(chunks) > 0
    assert all(isinstance(chunk, str) for chunk in chunks)
```

### Test d'Int√©gration API

```python
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_ask_endpoint():
    response = client.post("/ask", json={
        "question": "Quels sont mes meilleurs posts ?",
        "mode": "content_analyst",
        "n_posts": 3
    })

    assert response.status_code == 200
    data = response.json()
    assert "response" in data
    assert data["mode"] == "content_analyst"

def test_invalid_mode():
    response = client.post("/ask", json={
        "question": "Test",
        "mode": "invalid_mode"
    })

    assert response.status_code == 400
```

---

## üìä Monitoring et Logs

```python
import logging
import time

# Configurer le logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.post("/ask")
async def ask(req: QuestionRequest):
    start_time = time.time()

    logger.info(f"üì• Requ√™te re√ßue - Mode: {req.mode}, Question: {req.question[:50]}...")

    try:
        result = generate_with_mode(...)

        duration = time.time() - start_time
        logger.info(f"‚úÖ R√©ponse g√©n√©r√©e en {duration:.2f}s - Mode: {req.mode}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Erreur - Mode: {req.mode}, Erreur: {str(e)}")
        raise
```

---

## üîë Points Cl√©s √† Retenir

1. **Initialisation :** Charger le RAG pipeline UNE SEULE FOIS au d√©marrage
2. **Modes :** 4 modes disponibles (content_analyst, monetization, strategy, audience)
3. **Streaming :** Utiliser `stream=True` pour une meilleure UX
4. **Performance :** Ajuster `n_results`, `max_tokens` selon le besoin
5. **Fonction recommand√©e :** `generate_with_mode()` pour le backend

---

## üìû Support

- **Documentation compl√®te :** `MODES_GUIDE.md`
- **Exemples d'utilisation :** `USAGE_MODES.md`
- **Architecture :** `INTEGRATION_PROMPTS.md`

---

**Bon d√©veloppement ! üöÄ**